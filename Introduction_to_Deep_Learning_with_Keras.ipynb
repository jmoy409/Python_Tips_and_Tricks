{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39143dab-5fdd-440d-ae03-078f5bccb781",
   "metadata": {},
   "source": [
    "# Keras\n",
    "- Deep Learning Framework\n",
    "- Enables fast experimenation, less code needed\n",
    "- Runs on top of other frameworks like TensorFlow, Theano, CNTK\n",
    "    - wrapper around a backend library, like TensorFlow, Theano, CNTK\n",
    "\n",
    "### Why use Neural Network in the first place?\n",
    "- Deep Learning performs Feature Extraction and Classification at the same time\n",
    "    - DN can perform Feature Engineering as well\n",
    "    - Deal with unstructured data\n",
    "          - images are unstructured data\n",
    "              - don't care about why the network knows its a cat or dog\n",
    "- Machine Learning, you'll have to do feature selection, then classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7feeb1-5779-4b65-b9fe-856e0f271e7d",
   "metadata": {},
   "source": [
    "### Hello nets!\n",
    "You're going to build a simple neural network to get a feeling of how quickly it is to accomplish this in Keras.\n",
    "\n",
    "You will build a network that takes two numbers as an input, passes them through a hidden layer of 10 neurons, and finally outputs a single non-constrained number.\n",
    "\n",
    "A non-constrained output can be obtained by avoiding setting an activation function in the output layer. This is useful for problems like regression, when we want our output to be able to take any non-constrained value.\n",
    "\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the Sequential model from tensorflow.keras.models and the Denselayer from tensorflow.keras.layers.\n",
    "Create an instance of the Sequential model.\n",
    "Add a 10-neuron hidden Dense layer with an input_shape of two neurons.\n",
    "Add a final 1-neuron output layer and summarize your model with summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883613a3-415d-47fb-b30d-8c5381c0ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Sequential model and Dense layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b12d8-79cd-486a-9f26-4a43c7e0332b",
   "metadata": {},
   "source": [
    "### Counting parameters\n",
    "You've just created a neural network. But you're going to create a new one now, taking some time to think about the weights of each layer. The Keras Dense layer and the Sequential model are already loaded for you to use.\n",
    "\n",
    "This is the network you will be creating:\n",
    "\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Instantiate a new Sequential() model.\n",
    "Add a Dense() layer with five neurons and three neurons as input.\n",
    "Add a final dense layer with one neuron and no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f4016-bb03-456c-a4a5-1e32315d4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()\n",
    "\n",
    "#There are 20 parameters, 15 from the connections of our inputs to our hidden layer and 5 from the bias weight of each neuron in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb41ac-4862-4bf3-890f-3b8d71797736",
   "metadata": {},
   "source": [
    "### Specifying a model\n",
    "You will build a simple regression model to predict the orbit of the meteor!\n",
    "\n",
    "Your training data consist of measurements taken at time steps from -10 minutes before the impact region to +10 minutes after. Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor orbit at that time step.\n",
    "\n",
    "Note that you can view this problem as approximating a quadratic function via the use of neural networks.\n",
    "\n",
    "\n",
    "This data is stored in two numpy arrays: one called time_steps , what we call features, and another called y_positions, with the labels. Go on and build your model! It should be able to predict the y positions for the meteor orbit at future time steps.\n",
    "\n",
    "Keras Sequential model and Dense layers are available for you to use.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate a Sequential model.\n",
    "Add a Dense layer of 50 neurons with an input shape of 1 neuron.\n",
    "Add two Dense layers of 50 neurons each and 'relu' activation.\n",
    "End your model with a Dense layer with a single neuron and no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170cc42-b281-4adc-bd75-18f1ecad7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c24b7-0664-4646-94b7-88c0122277cf",
   "metadata": {},
   "source": [
    "### Training\n",
    "You're going to train your first model in this course, and for a good cause!\n",
    "\n",
    "Remember that before training your Keras models you need to compile them. This can be done with the .compile() method. The .compile() method takes arguments such as the optimizer, used for weight updating, and the loss function, which is what we want to minimize. Training your model is as easy as calling the .fit() method, passing on the features, labels and a number of epochs to train for.\n",
    "\n",
    "The regression model you built in the previous exercise is loaded for you to use, along with the time_steps and y_positions data. Train it and evaluate it on this very same data, let's see if your model can learn the meteor's trajectory.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Compile your model making use of the 'adam' optimizer and 'mse' as your loss function.\n",
    "Fit your model using the features and labels for 30 epochs.\n",
    "Evaluate your model with the .evaluate() method, passing the features and labels used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbda03-ff76-47f5-87b7-eeb3ba719c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e792c-926a-425b-a7b7-37d7cd5aac01",
   "metadata": {},
   "source": [
    "### Predicting the orbit!\n",
    "You've already trained a model that approximates the orbit of the meteor approaching Earth and it's loaded for you to use.\n",
    "\n",
    "Since you trained your model for values between -10 and 10 minutes, your model hasn't yet seen any other values for different time steps. You will now visualize how your model behaves on unseen data.\n",
    "\n",
    "If you want to check the source code of plot_orbit, paste show_code(plot_orbit) into the console.\n",
    "\n",
    "Hurry up, the Earth is running out of time!\n",
    "\n",
    "Remember np.arange(x,y) produces a range of values from x to y-1. That is the [x, y) interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f628f-870e-4b0b-ab34-f9cdfc0d1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the eighty minute orbit\n",
    "eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "# Plot the eighty minute orbit \n",
    "plot_orbit(eighty_min_orbit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da61e0-dcba-4e4b-aef2-924be161bcbc",
   "metadata": {},
   "source": [
    "Your model fits perfectly to the scientists trajectory for time values between -10 to +10, the region where the meteor crosses the impact region, so we won't be hit! However, it starts to diverge when predicting for new values we haven't trained for. This shows neural networks learn according to the data they are fed with. Data quality and diversity are very important. You've barely scratched the surface of what neural networks can do. Are you prepared for the next chapter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2ed02-fc71-4bf5-9c49-8ed6b590fdbc",
   "metadata": {},
   "source": [
    "### Exploring dollar bills\n",
    "You will practice building classification models in Keras with the Banknote Authentication dataset.\n",
    "\n",
    "Your goal is to distinguish between real and fake dollar bills. In order to do this, the dataset comes with 4 features: variance,skewness,kurtosis and entropy. These features are calculated by applying mathematical operations over the dollar bill images. The labels are found in the dataframe's class column.\n",
    "\n",
    "\n",
    "A pandas DataFrame named banknotes is ready to use, let's do some data exploration!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import seaborn as sns.\n",
    "Use seaborn's pairplot() on banknotes and set hue to be the name of the column containing the labels.\n",
    "Generate descriptive statistics for the banknotes authentication data.\n",
    "Count the number of observations per label with .value_counts()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bdb07-3943-44a5-a5bd-bbfa2e130027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use pairplot and set the hue to be our class column\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations per class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd44bca-43c9-4553-885b-4a44cf8968a2",
   "metadata": {},
   "source": [
    "### A binary classification model\n",
    "Now that you know what the Banknote Authentication dataset looks like, we'll build a simple model to distinguish between real and fake bills.\n",
    "\n",
    "You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model's output will be a value constrained between 0 and 1.\n",
    "\n",
    "We will interpret this output number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it's a fake bill.\n",
    "\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the Sequential model and Dense layer from tensorflow.keras.\n",
    "Create a sequential model.\n",
    "Add a 4 neuron input layer with the input_shape parameter and a 1 neuron output layer with sigmoid activation.\n",
    "Compile your model using sgd as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe1acc-3285-4c2b-87ba-6b167d2a15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sequential model and dense layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde420aa-7f69-42f7-8ff5-3041189c8a49",
   "metadata": {},
   "source": [
    "### Is this dollar bill fake ?\n",
    "You are now ready to train your model and check how well it performs when classifying new bills! The dataset has already been partitioned into features: X_train & X_test, and labels: y_train & y_test.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Train your model for 20 epochs calling .fit(), passing in the training data.\n",
    "Check your model accuracy using the .evaluate() method on the test data.\n",
    "Print accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d939a7-4ea5-4553-abaa-6fe5f0189e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613048df-3d18-407f-a8f0-67be8c623b61",
   "metadata": {},
   "source": [
    "### A multi-class model: softman and binary_crossentropy\n",
    "You're going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart's x and y coordinates on the board.)\n",
    "\n",
    "This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes/labels are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the softmax activation function to achieve a total sum of probabilities of 1 over all competitors.\n",
    "\n",
    "The Sequential model and Dense layers are already imported for you to use.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate a Sequential model.\n",
    "Add 3 dense layers of 128, 64 and 32 neurons each.\n",
    "Add a final dense layer with as many neurons as competitors.\n",
    "Compile your model using categorical_crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfc239-26be-4155-93d8-22be7304f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b12316-593b-4d17-80e3-ef5786aca813",
   "metadata": {},
   "source": [
    "### Prepare your dataset\n",
    "In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as darts. Pandas is imported as pd. Let's prepare this dataset!\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "2\n",
    "Import to_categorical from tensorflow.keras.utils.\n",
    "Apply to_categorical() to your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69658413-a437-43da-9f7f-ae64c81491cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde40e7f-85bc-403b-aea7-e69a283ff15c",
   "metadata": {},
   "source": [
    "### Training on dart throwers\n",
    "Your model is now ready, just as your dataset. It's time to train!\n",
    "\n",
    "The coordinates features and competitors labels you just transformed have been partitioned into coord_train,coord_test and competitors_train,competitors_test.\n",
    "\n",
    "Your model is also loaded. Feel free to visualize your training data or model.summary() in the console.\n",
    "\n",
    "Let's find out who threw which dart just by looking at the board!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Train your model on the training data for 200 epochs.\n",
    "Evaluate your model accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be3a02-45db-4bbc-9568-a8db8a35e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307d163-d092-40e9-ac68-06418bb36618",
   "metadata": {},
   "source": [
    "### Softmax predictions\n",
    "Your recently trained model is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the softmax activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in coords_small_test and np.argmax()to check this out!\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Predict with your model on coords_small_test.\n",
    "Print the model predictions.\n",
    "\n",
    "Use np.argmax()to extract the index of the highest probable competitor from each pred vector in preds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734056-1d94-4c92-9896-a76a16c6b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb6909-bfa8-4732-a0ca-0f3d872f1bfd",
   "metadata": {},
   "source": [
    "Well done! As you've seen you can easily interpret the softmax output. This can also help you spot those observations where your network is less certain on which class to predict, since you can see the probability distribution among classes per prediction. Let's learn how to solve new problems with neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c414d-5201-4bdc-99b7-c9d46625658e",
   "metadata": {},
   "source": [
    "### Multi-Label Model\n",
    "- activation function will be sigmoid and nodes will be based on # of classes\n",
    "- also use binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799ac8b-4c9b-4b64-9f7d-0da67bde93a0",
   "metadata": {},
   "source": [
    "### An irrigation machine\n",
    "You're going to automate the watering of farm parcels by making an intelligent irrigation machine. Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes/labels are not mutually exclusive, you could water all, none or any combination of farm parcels based on the inputs.\n",
    "\n",
    "To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes each neuron in the output layer able to output a number between 0 and 1 independently.\n",
    "\n",
    "The Sequential() model and Dense() layers are ready to be used. It's time to build an intelligent irrigation machine!\n",
    "\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate a Sequential() model.\n",
    "Add a hidden layer of 64 neurons with as many input neurons as there are sensors and relu activation.\n",
    "Add an output layer with as many neurons as parcels and sigmoidactivation.\n",
    "Compile your model with the adam optimizer and binary_crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27495b-aa95-48db-a3be-744c9cdefa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# Compile your model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss = 'binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a873d07-24cc-4c7a-855c-f8a639394831",
   "metadata": {},
   "source": [
    "### Training with multiple labels\n",
    "An output of your multi-label model could look like this: [0.76 , 0.99 , 0.66 ]. If we round up probabilities higher than 0.5, this observation will be classified as containing all 3 possible labels [1,1,1]. For this particular problem, this would mean watering all 3 parcels in your farm is the right thing to do, according to the network, given the input sensor measurements.\n",
    "\n",
    "You will now train and predict with the model you just built. sensors_train, parcels_train, sensors_test and parcels_test are already loaded for you to use.\n",
    "\n",
    "Let's see how well your intelligent machine performs!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Train the model for 100 epochs using a validation_split of 0.2.\n",
    "Predict with your model using the test data.\n",
    "Round up your preds with np.round().\n",
    "Evaluate your model's accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e86f9-f00b-41fc-bcfe-f6c675f2561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Predict on sensors_test and round up the predictions\n",
    "preds = model.predict(sensors_test)\n",
    "preds_rounded = np.round(preds)\n",
    "\n",
    "# Print rounded preds\n",
    "print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "# Evaluate your model's accuracy on the test data\n",
    "accuracy = model.evaluate(sensors_test, parcels_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae58bc-b187-4e53-b5b3-6e6059da2c65",
   "metadata": {},
   "source": [
    "### The history callback\n",
    "The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary parameter inside the returned h_callback object with the corresponding keys.\n",
    "\n",
    "The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels now loaded as X_train, y_train, X_test, y_test. This time you will store the model's historycallback and use the validation_data parameter as it trains.\n",
    "\n",
    "You will plot the results stored in history with plot_accuracy() and plot_loss(), two simple matplotlib functions. You can check their code in the console by pasting show_code(plot_loss).\n",
    "\n",
    "Let's see the behind the scenes of our training!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Train your model on X_train and y_train, validate each epoch on X_test and y_test.\n",
    "Use plot_lossextracting lossand val_loss from h_callback.\n",
    "Use plot_accuracyextracting accuracyand val_accuracy from h_callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d26282-4f8b-4285-97fa-3d0233927e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save its history\n",
    "h_callback = model.fit(X_train, y_train, epochs = 25,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['accuracy'], h_callback.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6ce4b-beda-45cc-b761-3500c5271961",
   "metadata": {},
   "source": [
    "### Early stopping your model\n",
    "The early stopping callback is useful since it allows for you to stop the model training if it no longer improves after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model's callback parameter in the .fit() method.\n",
    "\n",
    "The model you built to detect fake dollar bills is loaded for you to train, this time with early stopping. X_train, y_train, X_test and y_test are also available for your use.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the EarlyStoppingcallback from tensorflow.keras.callbacks.\n",
    "Define a callback, monitor 'val_accuracy' with a patience of 5 epochs.\n",
    "Train your model using the early stopping callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b4e5d-2e36-4671-ab3a-bb038674d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the early stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor='val_accuracy', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks= [monitor_val_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8c88c-ef0d-4d40-829c-cf2739a1e5f6",
   "metadata": {},
   "source": [
    "### A combination of callbacks\n",
    "Deep learning models can take a long time to train, especially when you move to deeper architectures and bigger datasets. Saving your model every time it improves as well as stopping it when it no longer does allows you to worry less about choosing the number of epochs to train for. You can also restore a saved model anytime and resume training where you left it.\n",
    "\n",
    "The model training and validation data are available in your workspace as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "Use the EarlyStopping() and the ModelCheckpoint() callbacks so that you can go eat a jar of cookies while you leave your computer to work!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import both the EarlyStopping and ModelCheckpoint callbacks from tensorflow.keras.\n",
    "Create monitor_val_acc as an EarlyStopping callback that will monitor 'val_accuracy', with a patience of 3 epochs.\n",
    "Create model_checkpoint as a ModelCheckpointcallback, save the best model as best_banknote_model.hdf5.\n",
    "Fit your model providing a list with the defined callbacks and X_test and y_test as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be1a5e-f20c-4314-9fba-5dd5ea7177ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_accuracy', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "model_checkpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc, model_checkpoint],\n",
    "                    validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f04ca-f638-4f60-a5c1-d4dc27a794bd",
   "metadata": {},
   "source": [
    "You've learned a powerful callback combo! Nice moves! Now you always save the model that performed best, even if you early stopped at one that was already performing worse.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95b24b-9bb0-46c5-9133-756df7d9bf7e",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "- Loss Learning Curves tends to decrease as epochs go by (learning to minimize loss function)\n",
    "    - eventually converges as loss cant go any lower\n",
    "    \n",
    "- Accuracy Learning Curve tends to increase for each epoch, but eventually can't improve any further\n",
    "- Model overfitting: best to pick the point when train and test start to diverge\n",
    "    - If learning curve is unstable, may be due to optimizer, learning rate, batch-size, network architecture, weight intialization, etc\n",
    "    \n",
    "#first store model initial weights\n",
    "init_weights = model.get_weights()\n",
    "#initialize 2 lists for storing accuracies\n",
    "train_accs = []\n",
    "tests_accs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e50a6-7f29-4f40-8312-3bbbcaa3fe58",
   "metadata": {},
   "source": [
    "### Learning the digits\n",
    "You're going to build a model on the digits dataset, a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8x8 pixel handwritten digits from 0 to 9:\n",
    "\n",
    "\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
    "The dataset has already been partitioned into X_train, y_train, X_test, and y_test, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras to_categorical() function.\n",
    "\n",
    "Let's build this new model!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Add a Dense layer of 16 neurons with relu activation and an input_shape that takes the total number of pixels of the 8x8 digit image.\n",
    "Add a Dense layer with 10 outputs and softmax activation.\n",
    "Compile your model with adam, categorical_crossentropy, and accuracy metrics.\n",
    "Make sure your model works by predicting on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9fbba-d135-4e1a-89e2-ff19f330f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (8*8,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model works and can process input data\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a144fa-b8bf-46a4-bea0-89f66452dddb",
   "metadata": {},
   "source": [
    "### Is the model overfitting?\n",
    "Let's train the model you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the plot_loss() function code, paste this in the console: show_code(plot_loss)\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Train your model for 60 epochs, using X_test and y_test as validation data.\n",
    "Use plot_loss() passing loss and val_loss as extracted from the history attribute of the h_callback object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba87746-7a9c-49fb-9958-1d5340647731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e99c6-d192-40a3-8ba4-7963b89b0509",
   "metadata": {},
   "source": [
    "Awesome choice! This graph doesn't show overfitting but convergence. It looks like your model has learned all it could from the data and it no longer improves. The test loss, although higher than the training loss, is not getting worse, so we aren't overfitting to the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49b863-d639-4b42-b84f-eeb6b6bcc1ec",
   "metadata": {},
   "source": [
    "### Do we need more data?\n",
    "It's time to check whether the digits dataset model you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:\n",
    "\n",
    "The model you just built.\n",
    "X_train,y_train,X_test, and y_test.\n",
    "The initial_weights of your model, saved after using model.get_weights().\n",
    "A pre-defined list of training sizes: training_sizes.\n",
    "A pre-defined early stopping callback monitoring loss: early_stop.\n",
    "Two empty lists to store the evaluation results: train_accs and test_accs.\n",
    "Train your model on the different training sizes and evaluate the results on X_test. End by plotting the results with plot_results().\n",
    "\n",
    "The full code for this exercise can be found on the slides!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Get a fraction of the training data determined by the size we are currently evaluating in the loop.\n",
    "Set the model weights to the initial_weights with set_weights() and train your model on the fraction of training data using early_stop as a callback.\n",
    "Evaluate and store the accuracy for the training fraction and the test set.\n",
    "Call plot_results() passing in the training and test accuracies for each training size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e57142-fa5e-45a6-a77e-6b7192aa0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f5c44-d858-4e84-8e7d-2c4c02845489",
   "metadata": {},
   "source": [
    "Good job, that was a lot of code to understand! The results shows that your model would not benefit a lot from more training data, since the test set accuracy is already starting to flatten. It's time to look at activation funtions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2b12b-2212-442f-8572-60d81568141a",
   "metadata": {},
   "source": [
    "### Comparing activation functions\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function get_model('relu') returns a copy of this model and applies the 'relu' activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "X_train, y_train, X_test, y_test are ready for you to use when training your models.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Fill up the activation functions array with relu,leaky_relu, sigmoid, and tanh.\n",
    "Get a new model for each iteration with get_model() passing the current activation function as a parameter.\n",
    "Fit your model providing the train and validation_data, use 20 epochs and set verbose to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e83b2d-a1c0-4441-b09d-a90987fd5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc3a35-04ef-483d-b0cd-8242339f49a7",
   "metadata": {},
   "source": [
    "### Comparing activation functions II\n",
    "What you coded in the previous exercise has been executed to obtain theactivation_results variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.\n",
    "\n",
    "For every h_callback of each activation function in activation_results:\n",
    "\n",
    "The h_callback.history['val_loss'] has been extracted.\n",
    "The h_callback.history['val_accuracy'] has been extracted.\n",
    "Both are saved into two dictionaries: val_loss_per_function and val_acc_per_function.\n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use pd.DataFrame()to create a new DataFrame from the val_loss_per_function dictionary.\n",
    "Call plot() on the DataFrame.\n",
    "Create another pandas DataFrame from val_acc_per_function.\n",
    "Once again, plot the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c31fe-f52d-433d-b796-f894da5288c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25413019-da5f-4656-9cdb-d2f10ca5d390",
   "metadata": {},
   "source": [
    "You've plotted both: loss and accuracy curves. It looks like sigmoid activation worked best for this particular model as the hidden layer's activation function. It led to a model with lower validation loss and higher accuracy after 100 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7e684-454b-4c22-9d92-f941711038e7",
   "metadata": {},
   "source": [
    "### Batch size and batch normalization\n",
    "- typically, normalizing data prior to feeding in NN is better as distribution is centered around 0 and scales all features.\n",
    "- However, consecutive layers no longer benefit from normalization after updating weights from gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03800d99-3f60-4808-837f-867566f3b28a",
   "metadata": {},
   "source": [
    "### Changing batch sizes\n",
    "You've seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it's not representative of the entire training set.\n",
    "\n",
    "Let's see how different batch sizes affect the accuracy of a simple binary classification model that separates red from blue dots.\n",
    "\n",
    "You'll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch.\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "Use get_model() to get a new, already compiled, model, then train your model for 5 epochs with a batch_size of 1.\n",
    "\n",
    "Now train a new model with batch_size equal to the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc695ef2-0e13-4115-87d6-effa0bf64c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "#accuracy was 85%\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
    "print(\"\\n The accuracy when using the whole training set as batch-size was: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "# accuracy was 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfaefab-a95e-436e-b568-ddf822b9f544",
   "metadata": {},
   "source": [
    "Great work! You can see that accuracy is lower when using a batch size equal to the training set size. This is not because the network had more trouble learning the optimization function: Even though the same number of epochs were used for both batch sizes the number of resulting weight updates was very different!. With a batch of size the training set and 5 epochs we only get 5 updates total, each update computes and averaged gradient descent with all the training set observations. To obtain similar results with this batch size we should increase the number of epochs so that more weight updates take place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c30be0-590e-4ead-bccb-60c768587aa0",
   "metadata": {},
   "source": [
    "### Batch normalizing a familiar model\n",
    "Remember the digits dataset you trained in the first exercise of this chapter?\n",
    "\n",
    "\n",
    "A multi-class classification problem that you solved using softmax and 10 neurons in your output layer.\n",
    "\n",
    "You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar way.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import BatchNormalization from tensorflow.keras layers.\n",
    "Build your deep network model, use 50 neurons for each hidden layer adding batch normalization in between layers.\n",
    "Compile your model with stochastic gradient descent, sgd, as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e4dd7-169b-4986-b911-315cb1ae4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    " \n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    " \n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4936990-b701-40e6-a5e4-6908c17929f9",
   "metadata": {},
   "source": [
    "### Batch normalization effects\n",
    "Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let's see how two identical models with and without batch normalization compare.\n",
    "\n",
    "The model you just built batchnorm_model is loaded for you to use. An exact copy of it without batch normalization: standard_model, is available as well. You can check their summary() in the console. X_train, y_train, X_test, and y_test are also loaded so that you can train both models.\n",
    "\n",
    "You will compare the accuracy learning curves for both models plotting them with compare_histories_acc().\n",
    "\n",
    "You can check the function pasting show_code(compare_histories_acc) in the console.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Train the standard_model for 10 epochs passing in train and validation data, storing its history in h1_callback.\n",
    "Train your batchnorm_model for 10 epochs passing in train and validation data, storing its history in h2_callback.\n",
    "Call compare_histories_acc passing in h1_callback and h2_callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1351b17-ceb7-4fdf-8ead-2759f1264bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your standard model, storing its history\n",
    "h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    " \n",
    "# Train the batch normalized model you recently built, store its history\n",
    "h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    " \n",
    "# Call compare_acc_histories passing in both model histories\n",
    "compare_histories_acc(h1_callback, h2_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c63e9-8b9d-445a-b19e-4bb261997084",
   "metadata": {},
   "source": [
    "Outstanding! You can see that for this deep model batch normalization proved to be useful, helping the model obtain high accuracy values just over the first 10 training epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac730c13-1ee5-45b5-9603-5a3aef616119",
   "metadata": {},
   "source": [
    "### Preparing a model for tuning\n",
    "Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from tensorflow.keras.optimizers so that you can also change its learning rate parameter.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Set the learning rate of the Adam optimizer object to the one passed in the arguments.\n",
    "Set the hidden layers activations to the one passed in the arguments.\n",
    "Pass the optimizer and the binary cross-entropy loss to the .compile() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580c368-f494-4bdd-8f4c-d2d85d19c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "   \n",
    "    # Create an Adam optimizer with the given learning rate\n",
    "    opt = Adam(lr=learning_rate)\n",
    "     \n",
    "    # Create your binary classification model  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "     \n",
    "    # Compile your model with your optimizer, loss, and metrics\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6eb7b-f8ac-4029-be01-c05bab4e2318",
   "metadata": {},
   "source": [
    "### Tuning the model parameters\n",
    "It's time to try out different parameters on your model and see how well it performs!\n",
    "\n",
    "The create_model() function you built in the previous exercise is ready for you to use.\n",
    "\n",
    "Since fitting the RandomizedSearchCV object would take too long, the results you'd get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).\n",
    "\n",
    "You don't need to use the optional epochs and batch_size parameters when building your KerasClassifier object since you are passing them as params to the random search and this works already.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import KerasClassifier from tensorflow.keras scikit_learn wrappers.\n",
    "Use your create_model function when instantiating your KerasClassifier.\n",
    "Set 'relu' and 'tanh' as activation, 32, 128, and 256 as batch_size, 50, 100, and 200 epochs, and learning_rate of 0.1, 0.01, and 0.001.\n",
    "Pass your converted model and the chosen params as you build your RandomizedSearchCV object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6ee54-acab-4a11-8dc2-5e069069d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from tensorflow.keras scikit learn wrappers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    " \n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    " \n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    " \n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    " \n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce942ca1-4bf3-429f-af5c-efe580274004",
   "metadata": {},
   "source": [
    "### Training with cross-validation\n",
    "Time to train your model with the best parameters found: 0.001 for the learning rate, 50 epochs, a 128 batch_size and relu activations.\n",
    "\n",
    "The create_model() function from the previous exercise is ready for you to use. X and y are loaded as features and labels.\n",
    "\n",
    "Use the best values found for your model when creating your KerasClassifier object so that they are used when performing cross_validation.\n",
    "\n",
    "End this chapter by training an awesome tuned model on the breast cancer dataset!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import KerasClassifier from tensorflow.keras scikit_learn wrappers.\n",
    "Create a KerasClassifier object providing the best parameters found.\n",
    "Pass your model, features and labels to cross_val_score to perform cross-validation with 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee58ab-7185-46ba-8f91-70bcdd718ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = .001, activation = 'relu'), epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336b4e5-da89-4221-85eb-dec8991fdb9f",
   "metadata": {},
   "source": [
    "### Tensor, Layers, and autoencoders\n",
    "- tensors are the main data structures used in deep learning, inputs, and outputs\n",
    "- tensor is a multidimensional array of numbers\n",
    "\n",
    "\n",
    "### Autoencoders\n",
    "- Autoencoders are models that aim at producing the same inputs as outputs\n",
    "- in the hidden layer, we decrease the number of neurons, we are effectively making our network learn to compress its inputs into a small set of neurons\n",
    "\n",
    "### Autoencoder use cases:\n",
    "- dimensionality reduction: smaller dimensional space representation of our inputs\n",
    "- de-noising data: if trained with clean data, irrelevant noise will be filtered out during reconstruction\n",
    "- anomaly detection: a poor reconstruction will result when the model is fed with unseen inputs, measure this as a loss\n",
    "\n",
    "### Building Autoencoder\n",
    "- build DNN first and compile it\n",
    "- add the encoder to inputs at the first layer\n",
    "- encoder = Sequential()\n",
    "- encoder.add(autoencoder.layers[0])\n",
    "- new model predictions returns he 4 numbers given by the 4 neurons of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d384810-1b09-419b-8aad-03a7d0f8c901",
   "metadata": {},
   "source": [
    "### It's a flow of tensors\n",
    "If you have already built a model, you can use the model.layers and the tensorflow.keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor.\n",
    "\n",
    "This is a useful tool when we want to obtain the output of a network at an intermediate layer.\n",
    "\n",
    "For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor.\n",
    "\n",
    "So that's what you're going to do right now!\n",
    "\n",
    "X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import tensorflow.keras.backend as K.\n",
    "Use the model.layers list to get a reference to the input and output of the first layer.\n",
    "Use K.function() to define a function that maps inp to out.\n",
    "Print the results of passing X_test through the 1st layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da655589-ba76-4168-a7fa-457455100b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow.keras backend\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebe49d-9ea9-4402-a139-0abdeaa56599",
   "metadata": {},
   "source": [
    "That took a while! If you take a look at the graphs you can see how the neurons are learning to spread out the inputs based on whether they are fake or legit dollar bills. (A single fake dollar bill is represented as a purple dot in the graph) At the start the outputs are closer to each other, the weights are learned as epochs go by so that fake and legit dollar bills get a different, further and further apart output. Click in between the graphs fast, it's like a movie!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb1f06-3387-4490-a2f8-029c454258f4",
   "metadata": {},
   "source": [
    "### Building an autoencoder\n",
    "Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded. The model then learns to decode it back to its original form.\n",
    "\n",
    "You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels (28 x 28). The autoencoder will essentially learn to turn the 784 pixels original image into a compressed 32 pixels image and learn how to use that encoded representation to bring back the original 784 pixels image.\n",
    "\n",
    "The Sequential model and Dense layers are ready for you to use.\n",
    "\n",
    "Let's build an autoencoder!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a Sequential model.\n",
    "Add a dense layer with as many neurons as the encoded image dimensions and input_shape the number of pixels in the original image.\n",
    "Add a final layer with as many neurons as pixels in the input image.\n",
    "Compile your autoencoder using adadelta as an optimizer and binary_crossentropy loss, then summarise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058984c5-c579-46c0-ad36-ea1f5e873a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784,), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd7b22-2401-4142-8be6-3b2af15484c5",
   "metadata": {},
   "source": [
    "## De-noising like an autoencoder\n",
    "Okay, you have just built an autoencoder model. Let's see how it handles a more challenging task.\n",
    "\n",
    "First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings(). To build the encoder you will make use of your autoencoder, that has already being trained. You will just use the first half of the network, which contains the input and the bottleneck output. That way, you will obtain a 32 number output which represents the encoded version of the input image.\n",
    "\n",
    "Then, you will apply your autoencoder to noisy images from MNIST, it should be able to clean the noisy artifacts.\n",
    "\n",
    "X_test_noise is loaded in your workspace. The digits in this noisy dataset look like this:\n",
    "\n",
    "\n",
    "\n",
    "Apply the power of the autoencoder!\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Build an encoder model with the first layer of your trained autoencoder model.\n",
    "Predict on X_test_noise with your encoder and show the results with show_encodings().\n",
    "\n",
    "Predict on X_test_noise with your autoencoder, this will effectively perform both the encoding and decoding.\n",
    "Plot noisy vs decoded images with compare_plot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922670f7-3e44-48b8-8754-c7ec60d402b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a66776-515b-4d26-9e33-5f895ac61314",
   "metadata": {},
   "source": [
    "Amazing! The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. I hope this new model opened up your mind to the many possible architectures and non-classical ML problems that neural networks can solve :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904002f-d704-48a0-9d69-1a95b4abff79",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks - computer vision\n",
    "- convolution model uses convolutional layers\n",
    "     - convolution is a simple mathematical operation that preserves spatial relationships\n",
    "     - when applied to images, it can detect relevant areas of interest like edges, corners, vertical lines\n",
    "     - input_shape(width, height, channels)\n",
    "         color has 3 channels (RGB), black and white has 2\n",
    "         \n",
    "- To use CNN in Keras, will first import the Conv2d layer and flatten from tensorflow keras layers\n",
    "- from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "- filters=32: this convolutional layer will learn 32 convolutional masks       \n",
    "- kernal_size=3: These masks will be 3x3 as defined in kernal size\n",
    "- input_shape=(28, 28, 1): For 28*28 black and white images with only one channel.\n",
    "\n",
    "- flatten he output of the previous layer\n",
    "- model.add(Flatten())\n",
    "\n",
    "- end this multiclass model with 3 outputs and softmax\n",
    "- model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "### Using pre-trained models to classify images, we first have to adapt these images so they can be understood by the model:\n",
    "- To prepare images for ResNet50, we will do the following:\n",
    "- from tensorflow.keras.preprocessing import image\n",
    "- from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "- then load image with load_img, providing the target size, for this particular model, it is 224 * 224.\n",
    "- turn image into a numpy array using img_to_array\n",
    "- expand dimensions of the array, then preprocess the input in he same way the training images were.\n",
    "- Then predict on our image and decode the predictions (getting the predicted classes with the highest probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a015c8b-4156-4ce1-9e0f-50b9bab530dd",
   "metadata": {},
   "source": [
    "### Building a CNN model\n",
    "Building a CNN model in Keras isn't much more difficult than building any of the models you've already built throughout the course! You just need to make use of convolutional layers.\n",
    "\n",
    "You're going to build a shallow convolutional model that classifies the MNIST digits dataset. The same one you de-noised with your autoencoder! The images are 28 x 28 pixels and just have one channel, since they are black and white pictures.\n",
    "\n",
    "Go ahead and build this small convolutional model!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the Conv2D and Flatten layers and instantiate your model.\n",
    "Add a first convolutional layer with 32 filters of size 3x3 and the corresponding 3D tuple as input_shape.\n",
    "Add a second convolutional layer with 16 filters of size 3x3 with relu activation.\n",
    "Flatten the previous layer output to create a one-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1f264-68cb-4515-80b9-0ce402a06974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from tensorflow.keras.layers import Conv2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa463e10-a5d6-4663-8f8e-5f93b4ba97fe",
   "metadata": {},
   "source": [
    "### Looking at convolutions\n",
    "Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime!\n",
    "\n",
    "To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The outputs you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image.\n",
    "\n",
    "The convolutional model you built in the previous exercise has already been trained for you. It can now correctly classify MNIST handwritten images. You can check it with model.summary() in the console.\n",
    "\n",
    "Let's look at the convolutional masks that were learned in the first convolutional layer of this model!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Obtain a reference to the outputs of the first convolutional layer in the model.\n",
    "Build a new model using the model's first layer input and the first_layer_output as outputs.\n",
    "Use this first_layer_model to predict on X_test.\n",
    "Plot the activations of the first digit of X_test for the 15th and the 18th neuron filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89328a-b7f7-4163-a29c-bac75e02786c",
   "metadata": {},
   "source": [
    "### Looking at convolutions\n",
    "Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime!\n",
    "\n",
    "To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The outputs you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image.\n",
    "\n",
    "The convolutional model you built in the previous exercise has already been trained for you. It can now correctly classify MNIST handwritten images. You can check it with model.summary() in the console.\n",
    "\n",
    "Let's look at the convolutional masks that were learned in the first convolutional layer of this model!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Obtain a reference to the outputs of the first convolutional layer in the model.\n",
    "Build a new model using the model's first layer input and the first_layer_output as outputs.\n",
    "Use this first_layer_model to predict on X_test.\n",
    "Plot the activations of the first digit of X_test for the 15th and the 18th neuron filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77df974-45f0-4837-85ee-a849c7ad2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59548e-324a-444a-ab94-fc32f4030417",
   "metadata": {},
   "source": [
    "Hurrah! Each neuron filter of the first layer learned a different convolution. The 15th filter (a.k.a convolutional mask) learned to detect horizontal traces in your digits. On the other hand, filter 18th seems to be checking for vertical traces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d79bb8-7400-4331-ae06-9de525ee4435",
   "metadata": {},
   "source": [
    "### Preparing your input image\n",
    "The original ResNet50 model was trained with images of size 224 x 224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. You need to pre-process the images you want to predict on in the same way.\n",
    "\n",
    "When predicting on a single image you need it to fit the model's input shape, which in this case looks like this: (batch-size, width, height, channels),np.expand_dims with parameter axis = 0 adds the batch-size dimension, representing that a single image will be passed to predict. This batch-size dimension value is 1, since we are only predicting on one image.\n",
    "\n",
    "You will go over these preprocessing steps as you prepare this dog's (named Ivy) image into one that can be classified by ResNet50.\n",
    "\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import image from tensorflow.keras.preprocessing and preprocess_input from tensorflow.keras.applications.resnet50.\n",
    "Load the image with the right target_size for your model.\n",
    "Turn it into an array with image.img_to_array().\n",
    "Pre-process img_expanded the same way the original ResNet50 training images were processed with preprocess_input()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de176760-99f9-4644-8e11-2c778f5433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image and preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fd53d-8920-43cc-ac50-00f6e0c13aef",
   "metadata": {},
   "source": [
    "Alright! Ivy is now ready for ResNet50. Do you know this dog's breed? Let's see what this model thinks it is!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e096b-7481-4962-a6a3-ca4b017e8b89",
   "metadata": {},
   "source": [
    "### Using a real world model\n",
    "Okay, so Ivy's picture is ready to be used by ResNet50. It is stored in img_ready and now looks like this:\n",
    "\n",
    "\n",
    "ResNet50 is a model trained on the Imagenet dataset that is able to distinguish between 1000 different labeled objects. ResNet50 is a deep model with 50 layers, you can check it in 3D here.\n",
    "\n",
    "ResNet50 and decode_predictions have both been imported from tensorflow.keras.applications.resnet50 for you.\n",
    "\n",
    "It's time to use this trained model to find out Ivy's breed!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instantiate a ResNet50 model, setting the weights parameter to be 'imagenet'.\n",
    "Use the model to predict on your processed image.\n",
    "Decode the first 3 predictions with decode_predictions()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761dd9e-a24a-4357-841e-e7bca5100788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc3ee2-3d72-4d19-bb1f-5bb7ac95cbd0",
   "metadata": {},
   "source": [
    "### Amazing! Check the console! Now you know Ivy is quite probably a Beagle and that deep learning models that have already been trained for you are easy to use!\n",
    "- Predicted: [('n02088364', 'beagle', 0.8280003), ('n02089867', 'Walker_hound', 0.12915272), ('n02089973', 'English_foxhound', 0.03711732)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937ec99-3c97-4266-8350-71231fcbd544",
   "metadata": {},
   "source": [
    "### Intro to LSTMs (Long Short Term Memory)\n",
    "- Type of recurrent Neural Network (RNN)\n",
    "- Can use past predictions in order to infer new ones\n",
    "    - Helps us solve problems where there is a dependence on past inputs\n",
    "    - They learn what to ignore, what to keep and select the most important pieces of past information to predict the future\n",
    "    \n",
    "    \n",
    "### When to use LSTMs\n",
    "- LSTMs have been used for image captioning, speech to text, text translation, text generalization, musical composition\n",
    "- Typically used to predict the next word in a sentence (google search)\n",
    "\n",
    "### Neural Networks can only deal with numbers, not text\n",
    "- need to transform each unique word into a number\n",
    "- numbers can then be used as inputs to an embedding layer\n",
    "\n",
    "#### Embedding layers learn to represent words as vectors of a predetermined size\n",
    "\n",
    "- With a sequence length of 3, we will end up feeding our model with two words and it will predict the third one.\n",
    "- First we must break up a sentence into at list of strings containing 3 words\n",
    "\n",
    "### After that, we turn our text sequences into numbers\n",
    "- each word is a index when using the list we looked at\n",
    "- vocab_size is + 1, because we account for 0 as an index reserved for special characters. Dictionary starts at 1, not 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da3145-5e1f-4cda-a17a-2915983527cc",
   "metadata": {},
   "source": [
    "### Text prediction with LSTMs\n",
    "During the following exercises you will build a toy LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable.\n",
    "\n",
    "You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model!\n",
    "\n",
    "The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Split the text into an array of words using .split().\n",
    "Make sentences of 4 words each, moving one word at a time.\n",
    "Instantiate a Tokenizer(), then fit it on the sentences with .fit_on_texts().\n",
    "Turn sentences into a sequence of numbers calling .texts_to_sequences()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c16c3-4036-40f6-a06f-844f5f2b9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd26b-c415-430d-84dd-b616960af250",
   "metadata": {},
   "source": [
    "#### Build your LSTM model\n",
    "You've already prepared your sequences of text. It's time to build your LSTM model!\n",
    "\n",
    "Remember your sequences had 4 words each, your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into meaningful vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words.\n",
    "\n",
    "The size of the vocabulary of words (the unique number of words) is stored in vocab_size.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the Embedding, LSTM and Dense layer from tensorflow.keras layers.\n",
    "Add an Embedding() layer of the vocabulary size, that will turn words into 8 number vectors and receive sequences of length 3.\n",
    "Add a 32 neuron LSTM() layer.\n",
    "Add a hidden Dense() layer of 32 neurons and an output layer of vocab_size neurons with softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e4bd4-c877-4706-99c7-7e3ad8389ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Embedding, LSTM and Dense layer\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = vocab_size, input_length = 3, output_dim =8 ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e48df-2ac7-4f10-9336-d6b90a3f66cc",
   "metadata": {},
   "source": [
    "That's a nice looking model you've built! You'll see that this model is powerful enough to learn text relationships, we aren't using a lot of text in this tiny example and our sequences are quite short. This model is to be trained as usual, you would just need to compile it with an optimizer like adam and use crossentropy loss. This is because we have modeled this next word prediction task as a classification problem with all the unique words in our vocabulary as candidate classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2e706-c145-433d-9822-5c794aef901e",
   "metadata": {},
   "source": [
    "### Decode your predictions\n",
    "Your LSTM model has already been trained (details in the previous exercise success message) so that you don't have to wait. It's time to define a function that decodes its predictions. The trained model will be passed as a default parameter to this function.\n",
    "\n",
    "Since you are predicting on a model that uses the softmax function, numpy's argmax() can be used to obtain the index/position representing the most probable next word out of the output vector of probabilities.\n",
    "\n",
    "The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model's next word prediction (which is an integer) into the actual written word it represents.\n",
    "\n",
    "You're very close to experimenting with your model!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use texts_to_sequences() to turn the test_text parameter into a sequence of numbers.\n",
    "Get the model's next word prediction by passing in test_seq . The index/position representing the word with the highest probability is obtained by calling .argmax(axis=1)[0] on the numpy array of predictions.\n",
    "Return the word that maps to the prediction using the tokenizer's index_word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933d328-4986-4640-bd9d-9e75571c410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcda3cd-4a71-4edb-a7b9-313b88a37c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef1612-0c0c-4a40-abaa-edbe48b19d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d5edb-fb81-445e-acc1-853e2a562b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
